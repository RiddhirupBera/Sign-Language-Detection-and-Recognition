{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9217a1d",
   "metadata": {},
   "source": [
    "# CSE3013 J Component\n",
    "# Detection and Recognition of Sign Language\n",
    "### Soham Bhattacharyya (19BCE1199)\n",
    "### Riddhirup Bera (19BCE1169)\n",
    "### Pranjal Singh (19BCE1207)\n",
    "### Avihrik Basak (19BCE1202)\n",
    "### Pranish Shrestha (19BCE1758)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5129002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#works with tensorflow 2.3.0 if keras libraries are imported directly\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPool2D, Flatten, BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD, Adagrad # 3 most popular gradient descent functions\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e99cbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Enter key: 13\n",
    "# Code for Nothing: 255\n",
    "# Code for Escape: 27\n",
    "\n",
    "import winsound\n",
    "frequency = 300  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "\n",
    "background = None\n",
    "alpha = 0.5 \n",
    "'''A lower value for this variable means \n",
    "running average will be performed over a larger amount of \n",
    "previous frames and vice-versa.'''\n",
    "\n",
    "\n",
    "#ROI: Region of interest\n",
    "ROI = {'top':100,'bottom':300,'right':150,'left':350}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ef3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_background(frame, alpha):\n",
    "\n",
    "    global background\n",
    "    \n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    \n",
    "    '''The objective is to detect active objects from the difference obtained from \n",
    "    the reference frame and the current frame (background). We keep feeding each frame to the accumulateWeighted() function,\n",
    "    and the function keeps finding the averages of all frames. '''\n",
    "    \n",
    "    cv2.accumulateWeighted(frame, background, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7635d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hand(frame, threshold=25):\n",
    "    global background\n",
    "    \n",
    "    #absolute difference between hand frame and background frame\n",
    "    abdiff = cv2.absdiff(frame, background.astype(\"uint8\")) \n",
    "    cv2.imshow(\"Difference\", abdiff)\n",
    "    \n",
    "    _ , thresholded = cv2.threshold(abdiff, threshold, 255, cv2.THRESH_BINARY) #if pixel value >threshold, \n",
    "    #255 else 0 (Binary thresholding)\n",
    "\n",
    "    # detects change in the image color and marks it as contour. Detection of hand in front of the background\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) \n",
    "    # hierarchy: contains information about image topology; number of elements equal to number of contours\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        #sorts by contour area and finds contour with maximum area\n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea) \n",
    "        \n",
    "        return (thresholded, hand_segment_max_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0e482",
   "metadata": {},
   "source": [
    "### Collecting images for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02de842d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "breaking loop\n"
     ]
    }
   ],
   "source": [
    "element = 0 #initialising the element variable where the gesture label will be stored\n",
    "cam = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # flipping the frame to prevent inverted image of captured frame\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "    cv2.putText(frame_copy, \"Enter next choice of character\", (0, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.putText(frame_copy, \"Press Escape to exit\", (0, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.imshow(\"Sign Language Image Data Collector\", frame_copy)\n",
    "    num_frames = 0\n",
    "    ele0 = cv2.waitKey(1) & 0xFF\n",
    "    if ele0==27: #break loop for escape\n",
    "        print(\"breaking loop\")\n",
    "        break\n",
    "    element = chr(ele0)\n",
    "    #element = 'A'\n",
    "    num_imgs_taken = 0\n",
    "    print(chr(ele0)) if ele0!=255 else print(end=\"\")\n",
    "    while ele0!=255 and ele0!=13:\n",
    "        ret, frame = cam.read()\n",
    "\n",
    "        # flipping the frame to prevent inverted image of captured frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        roi = frame[ROI['top']:ROI['bottom'], ROI['right']:ROI['left']]\n",
    "\n",
    "        gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "        if num_frames < 60:\n",
    "            calc_background(gray_frame, alpha)\n",
    "            cv2.putText(frame_copy, \"Wait while background is being recorded\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "            \n",
    "        # Placing the hand in the Region of Interest (adjustment allowed till 300 frames)\n",
    "        elif num_frames <= 300: \n",
    "\n",
    "            hand = detect_hand(gray_frame)\n",
    "            \n",
    "            cv2.putText(frame_copy, \"Make gesture for '\"+str(element)+\"'\", (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            # Checking if hand is actually detected by counting number of contours detected\n",
    "            if hand is not None:\n",
    "                \n",
    "                thresholded, hand_segment = hand\n",
    "\n",
    "                # Draw contours around hand segment\n",
    "                cv2.drawContours(frame_copy, [hand_segment + (ROI['right'], ROI['top'])], -1, (255, 0, 0),1)\n",
    "                \n",
    "                cv2.putText(frame_copy, str(num_frames)+\" for \" + str(element), (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "                # Also display the thresholded image\n",
    "                cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "        \n",
    "        else: \n",
    "            \n",
    "            # Detecting contours in the hand\n",
    "            hand = detect_hand(gray_frame)\n",
    "            # Checking for hand\n",
    "            if hand is not None:\n",
    "                \n",
    "                # unpack the thresholded img and the max_contour\n",
    "                thresholded, hand_segment = hand\n",
    "\n",
    "                # Drawing contours around hand segment\n",
    "                cv2.drawContours(frame_copy, [hand_segment + (ROI['right'], ROI['top'])], -1, (255, 0, 0),1)\n",
    "                \n",
    "                \n",
    "                cv2.putText(frame_copy, str(num_imgs_taken) + ' images for' + str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                \n",
    "                # Displaying the thresholded image\n",
    "                cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "                if num_imgs_taken <= 500:\n",
    "                    cv2.putText(frame_copy, 'Collected '+str(num_imgs_taken)+' training images for' + str(element), (0, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "#                     Path(\"data2/train/\"+str(element)+\"/\").mkdir(parents=True, exist_ok=True)\n",
    "#                     printed = cv2.imwrite(r\"data2/train/\"+str(element)+\"/\"+str(num_imgs_taken) + '.jpg', thresholded)\n",
    "#                     print(printed,\" img: \",num_imgs_taken ) if not printed else print(end=\"\")\n",
    "                    pass\n",
    "                elif num_imgs_taken<=600:\n",
    "                    cv2.putText(frame_copy, 'Collected '+str(num_imgs_taken)+' testing images for' + str(element), (0, 450), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "#                     Path(\"data2/test/\"+str(element)+\"/\").mkdir(parents=True, exist_ok=True)\n",
    "#                     printed = cv2.imwrite(r\"data2/test/\"+str(element)+\"/\"+str(num_imgs_taken) + '.jpg', thresholded)\n",
    "#                     print(printed,\" img: \",num_imgs_taken ) if not printed else print(end=\"\")\n",
    "                    pass\n",
    "                else:\n",
    "                    break\n",
    "                num_imgs_taken+=1\n",
    "            else:\n",
    "                cv2.putText(frame_copy, 'No hand detected...', (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "        # Drawing ROI on frame copy\n",
    "        cv2.rectangle(frame_copy, (ROI['left'], ROI['top']), (ROI['right'], ROI['bottom']), (0,255,168), 3)\n",
    "        \n",
    "        cv2.putText(frame_copy, \"CSE3013 J Component - Sign Language Detection & Recognition\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "        \n",
    "        # increment the number of frames for tracking\n",
    "        num_frames += 1\n",
    "\n",
    "        # Display the frame with segmented hand\n",
    "        cv2.imshow(\"Sign Language Image Data Collector\", frame_copy)\n",
    "\n",
    "        # Closing windows with Escape key\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    \n",
    "\n",
    "    #f = False\n",
    "# Releasing camera & destroying all the windows...\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb55d160",
   "metadata": {},
   "source": [
    "### Training CNN with 64 * 64 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1949d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12024 images belonging to 24 classes.\n",
      "Found 2400 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = r'data\\train'\n",
    "test_path = r'data\\test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=5,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=5, shuffle=True)\n",
    "\n",
    "imgs, labels = next(train_batches) # returns the next 5 images in the shuffled training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab55ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "64\n",
      "64\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 classes\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = imgs\n",
    "for i in range(imgs.ndim):\n",
    "    print(len(x))\n",
    "    x = x[0]\n",
    "x = imgs[0]\n",
    "fig, ax = plt.subplots()\n",
    "img = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "ax.imshow(img)\n",
    "plt.show()\n",
    "print(len(labels[0]),\"classes\")\n",
    "classes = len(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745b5e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Plotting the images\n",
    "classes = len(labels[0])\n",
    "def showBatch1(arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "showBatch1(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f52d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() #initialising a sequential model to store layers of tensors\n",
    "\n",
    "'''Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input \n",
    "which helps produce a tensor of outputs.\n",
    "\n",
    "Kernel: In image processing kernel is a convolution matrix or a mask which can be used for blurring, \n",
    "sharpening, embossing, edge detection, and more by doing a convolution between a kernel and an image.'''\n",
    "\n",
    "model.add(Conv2D(activation='relu', input_shape=(64,64,3), filters=32, kernel_size=(3, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(activation='relu', padding = 'same',filters=64, kernel_size=(3, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(activation='relu', padding = 'valid',filters=128, kernel_size=(3, 3) ))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(24,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efec5ad9",
   "metadata": {},
   "source": [
    "#### Adam gradient descent\n",
    "\n",
    "Validation accuracy: 96.42%\n",
    "6 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c9add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1203/1203 [==============================] - 326s 271ms/step - loss: 0.2218 - accuracy: 0.9618 - val_loss: 0.3758 - val_accuracy: 0.9371\n",
      "Epoch 2/6\n",
      "1203/1203 [==============================] - 65s 54ms/step - loss: 0.0878 - accuracy: 0.9837 - val_loss: 0.7718 - val_accuracy: 0.9125\n",
      "Epoch 3/6\n",
      "1203/1203 [==============================] - 70s 58ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.1368 - val_accuracy: 0.9567\n",
      "Epoch 4/6\n",
      "1203/1203 [==============================] - 70s 58ms/step - loss: 4.4047e-04 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9621\n",
      "Epoch 5/6\n",
      "1203/1203 [==============================] - 73s 61ms/step - loss: 3.2032e-04 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9658\n",
      "Epoch 6/6\n",
      "1203/1203 [==============================] - 68s 56ms/step - loss: 6.3842e-05 - accuracy: 1.0000 - val_loss: 0.1281 - val_accuracy: 0.9642\n",
      "loss of 0.0025481190532445908; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=6, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e7dab",
   "metadata": {},
   "source": [
    "#### SGD gradient descent\n",
    "\n",
    "Validation accuracy: 96.71%, 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59368721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1203/1203 [==============================] - 63s 52ms/step - loss: 0.3119 - accuracy: 0.9384 - val_loss: 0.0695 - val_accuracy: 0.9729\n",
      "Epoch 2/6\n",
      "1203/1203 [==============================] - 67s 56ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0956 - val_accuracy: 0.9658\n",
      "Epoch 3/6\n",
      "1203/1203 [==============================] - 67s 56ms/step - loss: 9.5415e-04 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9671\n",
      "loss of 0.0018245566170662642; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=6, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40e9d5",
   "metadata": {},
   "source": [
    "#### Adagrad gradient descent\n",
    "\n",
    "Validation accuracy: 95.79%, 6 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5fef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1203/1203 [==============================] - 61s 51ms/step - loss: 0.2836 - accuracy: 0.9456 - val_loss: 0.1660 - val_accuracy: 0.9538\n",
      "Epoch 2/6\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.1503 - val_accuracy: 0.9513\n",
      "Epoch 3/6\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9571\n",
      "Epoch 4/6\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9571\n",
      "Epoch 5/6\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 8.2463e-04 - accuracy: 1.0000 - val_loss: 0.1243 - val_accuracy: 0.9583\n",
      "Epoch 6/6\n",
      "1203/1203 [==============================] - 67s 55ms/step - loss: 6.2254e-04 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9579\n",
      "loss of 0.0007578865042887628; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adagrad\n",
    "model1 = tf.keras.models.clone_model(\n",
    "    model, input_tensors=None, clone_function=None\n",
    ")\n",
    "model1.compile(optimizer=Adagrad(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model1.fit(train_batches, epochs=6, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model1.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model1.metrics_names[0]} of {scores[0]}; {model1.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b8f4a4",
   "metadata": {},
   "source": [
    "#### SGD gradient descent \n",
    "Validation accuracy: 94.29%, 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7e55ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1203/1203 [==============================] - 343s 285ms/step - loss: 0.3556 - accuracy: 0.9120 - val_loss: 0.2789 - val_accuracy: 0.9388\n",
      "Epoch 2/10\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2649 - val_accuracy: 0.9429\n",
      "Epoch 3/10\n",
      "1203/1203 [==============================] - 68s 56ms/step - loss: 6.0521e-04 - accuracy: 1.0000 - val_loss: 0.2620 - val_accuracy: 0.9429\n",
      "Epoch 4/10\n",
      "1203/1203 [==============================] - 73s 61ms/step - loss: 3.9687e-04 - accuracy: 1.0000 - val_loss: 0.2694 - val_accuracy: 0.9421\n",
      "Epoch 5/10\n",
      "1203/1203 [==============================] - 76s 63ms/step - loss: 3.0564e-04 - accuracy: 1.0000 - val_loss: 0.2663 - val_accuracy: 0.9429\n",
      "loss of 0.02056077867746353; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2008942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('cse4020_CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffcf539",
   "metadata": {},
   "source": [
    "### Training CNN with 128 * 128 pixel images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a9208eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12024 images belonging to 24 classes.\n",
      "Found 2400 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path1 = r'data2\\train'\n",
    "test_path1 = r'data2\\test'\n",
    "train_batches1 = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path1, target_size=(128,128), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches1 = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path1, target_size=(128,128), class_mode='categorical', batch_size=10, shuffle=True)\n",
    "imgs1, labels1 = next(train_batches1) # returns the next 5 images in the shuffled training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf5788c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 128, 128, 3)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "showBatch1(imgs1)\n",
    "print(imgs1.shape)\n",
    "print(labels1[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81311586",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx = Sequential() #initialising a sequential model to store layers of tensors\n",
    "\n",
    "'''Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input \n",
    "which helps produce a tensor of outputs.\n",
    "\n",
    "Kernel: In image processing kernel is a convolution matrix or a mask which can be used for blurring, \n",
    "sharpening, embossing, edge detection, and more by doing a convolution between a kernel and an image.'''\n",
    "\n",
    "modelx.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "modelx.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "modelx.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "modelx.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "modelx.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "modelx.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "modelx.add(Flatten())\n",
    "modelx.add(Dense(128,activation =\"relu\"))\n",
    "modelx.add(Dense(256,activation =\"relu\"))\n",
    "modelx.add(Dense(256,activation =\"relu\"))\n",
    "modelx.add(Dense(classes,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d859220",
   "metadata": {},
   "source": [
    "#### SGD Gradient Descent\n",
    "Validation accuracy: 96.96%, 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a310466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1203/1203 [==============================] - 818s 680ms/step - loss: 0.2421 - accuracy: 0.9705 - val_loss: 0.1099 - val_accuracy: 0.9692\n",
      "Epoch 2/3\n",
      "1203/1203 [==============================] - 808s 672ms/step - loss: 6.6163e-04 - accuracy: 1.0000 - val_loss: 0.1030 - val_accuracy: 0.9704\n",
      "Epoch 3/3\n",
      "1203/1203 [==============================] - 732s 609ms/step - loss: 3.2849e-04 - accuracy: 1.0000 - val_loss: 0.1025 - val_accuracy: 0.9696\n",
      "loss of 0.00673780869692564; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "modelx.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = modelx.fit(train_batches1, epochs=3, callbacks=[reduce_lr, early_stop],  validation_data = test_batches1)#, checkpoint])\n",
    "imgs1, labels1 = next(train_batches1) # For getting next batch of train images\n",
    "\n",
    "imgs1, labels1 = next(test_batches1) # For getting next batch of test images\n",
    "scores = modelx.evaluate(imgs1, labels1, verbose=0)\n",
    "print(f'{modelx.metrics_names[0]} of {scores[0]}; {modelx.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dde3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelx.save('cse3013_CNN2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69006248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 126, 126, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 63, 63, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 29, 29, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               6422656   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 24)                6168      \n",
      "=================================================================\n",
      "Total params: 6,898,456\n",
      "Trainable params: 6,898,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelx.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ddc05",
   "metadata": {},
   "source": [
    "### Recording a key with sign language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f90c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "class_names = [x[0][-1] if x[0][-2]=='\\\\' else '' for x in os.walk('data2/train')][1:]\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e98ddbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(r\"cse3013_CNN2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164c7115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions\n",
      "\n",
      "1. To start storing new key, press 1\n",
      "    a) Let the camera accumulate the background\n",
      "    b) Make gesture\n",
      "    c) Once content with the output, press Enter to append the predicted character to the key being built\n",
      "    d) Repeat 'b' and 'c' to add more characters in the key\n",
      "    e) To delete a character, press Backspace\n",
      "    f) To store key thus formed, press Escape\n",
      "Press enter to continue\n",
      "\n",
      "\n",
      "breaking loop\n"
     ]
    }
   ],
   "source": [
    "print('Instructions')\n",
    "print(\"\"\"\n",
    "1. To start storing new key, press 1\n",
    "    a) Let the camera accumulate the background\n",
    "    b) Make gesture\n",
    "    c) Once content with the output, press Enter to append the predicted character to the key being built\n",
    "    d) Repeat 'b' and 'c' to add more characters in the key\n",
    "    e) To delete a character, press Backspace\n",
    "    f) To store key thus formed, press Escape\n",
    "Press enter to continue\n",
    "\"\"\")\n",
    "#1 if storing key\n",
    "x = input()\n",
    "string = ''\n",
    "cam = cv2.VideoCapture(0)\n",
    "num_frames =0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    # filpping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # ROI from the frame\n",
    "    roi = frame[ROI['top']:ROI['bottom'], ROI['right']:ROI['left']]\n",
    "\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "    cv2.putText(frame_copy, \"CSE3013 J Component - Sign Language Detection & Recognition\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "    cv2.rectangle(frame_copy, (ROI['left'], ROI['top']), (ROI['right'], ROI['bottom']), (255,128,0), 3)\n",
    "    \n",
    "    ele0 = cv2.waitKey(1) & 0xFF\n",
    "    if ele0==27: #break loop for escape\n",
    "        print(\"breaking loop\")\n",
    "        break\n",
    "    element = chr(ele0)\n",
    "    #element = 'A'\n",
    "    num_imgs_taken = 0\n",
    "    #print(chr(ele0),ele0) if ele0!=255 else print(end=\"\")\n",
    "    while chr(ele0)=='1':\n",
    "        ret, frame = cam.read()\n",
    "\n",
    "        # filpping the frame to prevent inverted image of captured frame...\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        frame_copy = frame.copy()\n",
    "\n",
    "        # ROI from the frame\n",
    "        roi = frame[ROI['top']:ROI['bottom'], ROI['right']:ROI['left']]\n",
    "\n",
    "        gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "\n",
    "\n",
    "        if num_frames < 70:\n",
    "\n",
    "            calc_background(gray_frame, alpha)\n",
    "\n",
    "            cv2.putText(frame_copy, \"Wait while Background is being recorded\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "\n",
    "        else: \n",
    "            # segmenting the hand region\n",
    "            hand = detect_hand(gray_frame)\n",
    "\n",
    "\n",
    "            # Checking for hand\n",
    "            if hand is not None:\n",
    "\n",
    "                thresholded, hand_segment = hand\n",
    "\n",
    "                # Drawing contours around hand segment\n",
    "                cv2.drawContours(frame_copy, [hand_segment + (ROI['right'], ROI['top'])], -1, (255, 0, 0),1)\n",
    "\n",
    "                cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "\n",
    "                thresholded = cv2.resize(thresholded, (128, 128))\n",
    "                thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "                thresholded = np.reshape(thresholded, (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "\n",
    "                pred = model.predict(thresholded)\n",
    "                cv2.putText(frame_copy, class_names[np.argmax(pred)], (170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                ele1=cv2.waitKey(1) & 0xFF\n",
    "                if ele1==13: #enter key\n",
    "                    string+=class_names[np.argmax(pred)]\n",
    "                    #print(\"Enter\",string)\n",
    "                if ele1==8 and len(string)>=1: #backspace\n",
    "                    string = string[:-1]\n",
    "                    #print(\"Backspace\",string)\n",
    "                if ele1==27:\n",
    "                    break\n",
    "            if len(string)>0:\n",
    "                cv2.putText(frame_copy, \"Key: \"+str('*'*(len(string)-1)+string[-1]), (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "\n",
    "        # Draw ROI on frame_copy\n",
    "        cv2.rectangle(frame_copy, (ROI['left'], ROI['top']), (ROI['right'], ROI['bottom']), (0,255,168), 3)\n",
    "        \n",
    "        # incrementing the number of frames for collecting background\n",
    "        if num_frames<=70:\n",
    "            num_frames += 1\n",
    "\n",
    "        # Display the frame with detected hand\n",
    "        cv2.putText(frame_copy, \"CSE3013 J Component - Sign Language Detection & Recognition\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "        cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "\n",
    "\n",
    "        # Close windows with Escape key\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if k == 27:\n",
    "            break\n",
    "    if len(string)>0:\n",
    "        #write to file\n",
    "        cv2.putText(frame_copy, \"Writing to file\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "        time.sleep(1)\n",
    "        file1 = open(\"key.txt\", \"w\") \n",
    "        file1.write(string)\n",
    "        file1.close()\n",
    "        string=\"\"\n",
    "    background=None\n",
    "    num_frames = 0\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera and destroy all the windows\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c15727",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee99fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
