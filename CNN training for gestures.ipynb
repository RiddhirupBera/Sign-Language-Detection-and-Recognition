{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5129002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1949d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12024 images belonging to 24 classes.\n",
      "Found 2400 images belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = r'data\\train'\n",
    "test_path = r'data\\test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=5,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=5, shuffle=True)\n",
    "\n",
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745b5e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 64, 64, 3)\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Plotting the images...\n",
    "import matplotlib.pyplot as plt\n",
    "classes = len(labels[0])\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f52d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() #initialising a sequential model to store layers of tensors\n",
    "\n",
    "'''Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input \n",
    "which helps produce a tensor of outputs.\n",
    "\n",
    "Kernel: In image processing kernel is a convolution matrix or a mask which can be used for blurring, \n",
    "sharpening, embossing, edge detection, and more by doing a convolution between a kernel and an image.'''\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(classes,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=6, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16621f8",
   "metadata": {},
   "source": [
    "### Finalised experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c9add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1203/1203 [==============================] - 326s 271ms/step - loss: 0.2218 - accuracy: 0.9618 - val_loss: 0.3758 - val_accuracy: 0.9371\n",
      "Epoch 2/6\n",
      "1203/1203 [==============================] - 65s 54ms/step - loss: 0.0878 - accuracy: 0.9837 - val_loss: 0.7718 - val_accuracy: 0.9125\n",
      "Epoch 3/6\n",
      "1203/1203 [==============================] - 70s 58ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.1368 - val_accuracy: 0.9567\n",
      "Epoch 4/6\n",
      "1203/1203 [==============================] - 70s 58ms/step - loss: 4.4047e-04 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9621\n",
      "Epoch 5/6\n",
      "1203/1203 [==============================] - 73s 61ms/step - loss: 3.2032e-04 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9658\n",
      "Epoch 6/6\n",
      "1203/1203 [==============================] - 68s 56ms/step - loss: 6.3842e-05 - accuracy: 1.0000 - val_loss: 0.1281 - val_accuracy: 0.9642\n",
      "loss of 0.0025481190532445908; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=6, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59368721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1203/1203 [==============================] - 63s 52ms/step - loss: 0.3119 - accuracy: 0.9384 - val_loss: 0.0695 - val_accuracy: 0.9729\n",
      "Epoch 2/6\n",
      "1203/1203 [==============================] - 67s 56ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0956 - val_accuracy: 0.9658\n",
      "Epoch 3/6\n",
      "1203/1203 [==============================] - 67s 56ms/step - loss: 9.5415e-04 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9671\n",
      "loss of 0.0018245566170662642; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=6, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b5fef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1203/1203 [==============================] - 61s 51ms/step - loss: 0.2836 - accuracy: 0.9456 - val_loss: 0.1660 - val_accuracy: 0.9538\n",
      "Epoch 2/6\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.1503 - val_accuracy: 0.9513\n",
      "Epoch 3/6\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9571\n",
      "Epoch 4/6\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9571\n",
      "Epoch 5/6\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 8.2463e-04 - accuracy: 1.0000 - val_loss: 0.1243 - val_accuracy: 0.9583\n",
      "Epoch 6/6\n",
      "1203/1203 [==============================] - 67s 55ms/step - loss: 6.2254e-04 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9579\n",
      "loss of 0.0007578865042887628; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adagrad\n",
    "model1 = tf.keras.models.clone_model(\n",
    "    model, input_tensors=None, clone_function=None\n",
    ")\n",
    "model1.compile(optimizer=Adagrad(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model1.fit(train_batches, epochs=6, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model1.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model1.metrics_names[0]} of {scores[0]}; {model1.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7e55ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1203/1203 [==============================] - 343s 285ms/step - loss: 0.3556 - accuracy: 0.9120 - val_loss: 0.2789 - val_accuracy: 0.9388\n",
      "Epoch 2/10\n",
      "1203/1203 [==============================] - 66s 55ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2649 - val_accuracy: 0.9429\n",
      "Epoch 3/10\n",
      "1203/1203 [==============================] - 68s 56ms/step - loss: 6.0521e-04 - accuracy: 1.0000 - val_loss: 0.2620 - val_accuracy: 0.9429\n",
      "Epoch 4/10\n",
      "1203/1203 [==============================] - 73s 61ms/step - loss: 3.9687e-04 - accuracy: 1.0000 - val_loss: 0.2694 - val_accuracy: 0.9421\n",
      "Epoch 5/10\n",
      "1203/1203 [==============================] - 76s 63ms/step - loss: 3.0564e-04 - accuracy: 1.0000 - val_loss: 0.2663 - val_accuracy: 0.9429\n",
      "loss of 0.02056077867746353; accuracy of 100.0%\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2008942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('cse4020_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cadf5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
